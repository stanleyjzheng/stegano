{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import cv2\n",
    "from skimage import io\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import albumentations as albu\n",
    "import matplotlib.pyplot as plt\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "import sklearn\n",
    "from timm.utils.agc import adaptive_clip_grad\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(SEED)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "for label, kind in enumerate(['Cover', 'JMiPOD', 'JUNIWARD', 'UERD']):\n",
    "    for path in glob('../input/alaska2-image-steganalysis/Cover/*.jpg'):\n",
    "        dataset.append({\n",
    "            'kind': kind,\n",
    "            'image_name': path.split('/')[-1],\n",
    "            'label': label\n",
    "        })\n",
    "\n",
    "random.shuffle(dataset)\n",
    "dataset = pd.DataFrame(dataset)\n",
    "\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "dataset.loc[:, 'fold'] = 0\n",
    "for fold_number, (train_index, val_index) in enumerate(gkf.split(X=dataset.index, y=dataset['label'], groups=dataset['image_name'])):\n",
    "    dataset.loc[dataset.iloc[val_index].index, 'fold'] = fold_number"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def get_train_transforms():\n",
    "    return albu.Compose([\n",
    "            albu.HorizontalFlip(p=0.5),\n",
    "            albu.VerticalFlip(p=0.5),\n",
    "            albu.Resize(height=512, width=512, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)\n",
    "\n",
    "def get_valid_transforms():\n",
    "    return albu.Compose([\n",
    "            albu.Resize(height=512, width=512, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT_PATH = '../input/alaska2-image-steganalysis'\n",
    "\n",
    "def onehot(size, target):\n",
    "    vec = torch.zeros(size, dtype=torch.float32)\n",
    "    vec[target] = 1.\n",
    "    return vec\n",
    "\n",
    "class DatasetRetriever(Dataset):\n",
    "\n",
    "    def __init__(self, kinds, image_names, labels, transforms=None):\n",
    "        super().__init__()\n",
    "        self.kinds = kinds\n",
    "        self.image_names = image_names\n",
    "        self.labels = labels\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        kind, image_name, label = self.kinds[index], self.image_names[index], self.labels[index]\n",
    "        image = cv2.imread(f'{DATA_ROOT_PATH}/{kind}/{image_name}', cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "        if self.transforms:\n",
    "            sample = {'image': image}\n",
    "            sample = self.transforms(**sample)\n",
    "            image = sample['image']\n",
    "            \n",
    "        target = onehot(4, label)\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.image_names.shape[0]\n",
    "\n",
    "    def get_labels(self):\n",
    "        return list(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_number = 0\n",
    "\n",
    "train_dataset = DatasetRetriever(\n",
    "    kinds=dataset[dataset['fold'] != fold_number].kind.values,\n",
    "    image_names=dataset[dataset['fold'] != fold_number].image_name.values,\n",
    "    labels=dataset[dataset['fold'] != fold_number].label.values,\n",
    "    transforms=get_train_transforms(),\n",
    ")\n",
    "\n",
    "validation_dataset = DatasetRetriever(\n",
    "    kinds=dataset[dataset['fold'] == fold_number].kind.values,\n",
    "    image_names=dataset[dataset['fold'] == fold_number].image_name.values,\n",
    "    labels=dataset[dataset['fold'] == fold_number].label.values,\n",
    "    transforms=get_valid_transforms(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_image(train_dataset, i):\n",
    "    image, target = train_dataset[i]\n",
    "    numpy_image = image.permute(1,2,0).cpu().numpy()\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
    "\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(numpy_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "        \n",
    "def alaska_weighted_auc(y_true, y_valid):\n",
    "    \"\"\"\n",
    "    stolen from https://www.kaggle.com/anokas/weighted-auc-metric-updated\n",
    "    \"\"\"\n",
    "\n",
    "    tpr_thresholds = [0.0, 0.4, 1.0]\n",
    "    weights = [2, 1]\n",
    "\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_true, y_valid, pos_label=1)\n",
    "\n",
    "    # size of subsets\n",
    "    areas = np.array(tpr_thresholds[1:]) - np.array(tpr_thresholds[:-1])\n",
    "\n",
    "    # The total area is normalized by the sum of weights such that the final weighted AUC is between 0 and 1.\n",
    "    normalization = np.dot(areas, weights)\n",
    "\n",
    "    competition_metric = 0\n",
    "    for idx, weight in enumerate(weights):\n",
    "        y_min = tpr_thresholds[idx]\n",
    "        y_max = tpr_thresholds[idx + 1]\n",
    "        mask = (y_min < tpr) & (tpr < y_max)\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        x_padding = np.linspace(fpr[mask][-1], 1, 100)\n",
    "\n",
    "        x = np.concatenate([fpr[mask], x_padding])\n",
    "        y = np.concatenate([tpr[mask], [y_max] * len(x_padding)])\n",
    "        y = y - y_min  # normalize such that curve starts at y=0\n",
    "        score = metrics.auc(x, y)\n",
    "        submetric = score * weight\n",
    "        competition_metric += submetric\n",
    "\n",
    "    return competition_metric / normalization\n",
    "        \n",
    "class RocAucMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.y_true = np.array([0,1])\n",
    "        self.y_pred = np.array([0.5,0.5])\n",
    "        self.score = 0\n",
    "\n",
    "    def update(self, y_true, y_pred):\n",
    "        y_true = y_true.cpu().numpy().argmax(axis=1).clip(min=0, max=1).astype(int)\n",
    "        y_pred = 1 - nn.functional.softmax(y_pred, dim=1).data.cpu().numpy()[:,0]\n",
    "        self.y_true = np.hstack((self.y_true, y_true))\n",
    "        self.y_pred = np.hstack((self.y_pred, y_pred))\n",
    "        self.score = alaska_weighted_auc(self.y_true, self.y_pred)\n",
    "    \n",
    "    @property\n",
    "    def avg(self):\n",
    "        return self.score\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"also stolen, I don't remember from where\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label smoothing\n",
    "class LabelSmoothing(nn.Module):\n",
    "    def __init__(self, smoothing = 0.05):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        if self.training:\n",
    "            x = x.float()\n",
    "            target = target.float()\n",
    "            logprobs = torch.nn.functional.log_softmax(x, dim = -1)\n",
    "\n",
    "            nll_loss = -logprobs * target\n",
    "            nll_loss = nll_loss.sum(-1)\n",
    "    \n",
    "            smooth_loss = -logprobs.mean(dim=-1)\n",
    "\n",
    "            loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n",
    "\n",
    "            return loss.mean()\n",
    "        else:\n",
    "            return torch.nn.functional.cross_entropy(x, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class Fitter:\n",
    "    \n",
    "    def __init__(self, model, device, config):\n",
    "        self.config = config\n",
    "        self.epoch = 0\n",
    "        \n",
    "        self.base_dir = './'\n",
    "        self.log_path = f'{self.base_dir}/log.txt'\n",
    "        self.best_summary_loss = 10**5\n",
    "\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "\n",
    "        param_optimizer = list(self.model.named_parameters())\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ] \n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=config.lr)\n",
    "        self.scheduler = config.SchedulerClass(self.optimizer, **config.scheduler_params)\n",
    "        self.criterion = LabelSmoothing().to(self.device)\n",
    "        self.log(f'Fitter prepared. Device is {self.device}')\n",
    "\n",
    "    def fit(self, train_loader, validation_loader):\n",
    "        for e in range(self.config.n_epochs):\n",
    "            if self.config.verbose:\n",
    "                lr = self.optimizer.param_groups[0]['lr']\n",
    "                timestamp = datetime.utcnow().isoformat()\n",
    "                self.log(f'\\n{timestamp}\\nLR: {lr}')\n",
    "\n",
    "            t = time.time()\n",
    "            summary_loss, final_scores = self.train_one_epoch(train_loader)\n",
    "\n",
    "            self.log(f'[RESULT]: Train. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, final_score: {final_scores.avg:.5f}, time: {(time.time() - t):.5f}')\n",
    "            self.save(f'{self.base_dir}/last-checkpoint.bin')\n",
    "\n",
    "            t = time.time()\n",
    "            summary_loss, final_scores = self.validation(validation_loader)\n",
    "\n",
    "            self.log(f'[RESULT]: Val. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, final_score: {final_scores.avg:.5f}, time: {(time.time() - t):.5f}')\n",
    "            if summary_loss.avg < self.best_summary_loss:\n",
    "                self.best_summary_loss = summary_loss.avg\n",
    "                self.model.eval()\n",
    "                self.save(f'{self.base_dir}/best-checkpoint-{str(self.epoch).zfill(3)}epoch.bin')\n",
    "                for path in sorted(glob(f'{self.base_dir}/best-checkpoint-*epoch.bin'))[:-3]:\n",
    "                    os.remove(path)\n",
    "\n",
    "            if self.config.validation_scheduler:\n",
    "                self.scheduler.step(metrics=summary_loss.avg)\n",
    "\n",
    "            self.epoch += 1\n",
    "\n",
    "    def validation(self, val_loader):\n",
    "        self.model.eval()\n",
    "        summary_loss = AverageMeter()\n",
    "        final_scores = RocAucMeter()\n",
    "        t = time.time()\n",
    "        for step, (images, targets) in enumerate(val_loader):\n",
    "            if self.config.verbose:\n",
    "                if step % self.config.verbose_step == 0:\n",
    "                    print(\n",
    "                        f'Val Step {step}/{len(val_loader)}, ' + \\\n",
    "                        f'summary_loss: {summary_loss.avg:.5f}, final_score: {final_scores.avg:.5f}, ' + \\\n",
    "                        f'time: {(time.time() - t):.5f}', end='\\r'\n",
    "                    )\n",
    "            with torch.no_grad():\n",
    "                targets = targets.to(self.device).float()\n",
    "                batch_size = images.shape[0]\n",
    "                images = images.to(self.device).float()\n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                final_scores.update(targets, outputs)\n",
    "                summary_loss.update(loss.detach().item(), batch_size)\n",
    "\n",
    "        return summary_loss, final_scores\n",
    "\n",
    "    def train_one_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "        summary_loss = AverageMeter()\n",
    "        final_scores = RocAucMeter()\n",
    "        t = time.time()\n",
    "        for step, (images, targets) in enumerate(train_loader):\n",
    "            if self.config.verbose:\n",
    "                if step % self.config.verbose_step == 0:\n",
    "                    print(\n",
    "                        f'Train Step {step}/{len(train_loader)}, ' + \\\n",
    "                        f'summary_loss: {summary_loss.avg:.5f}, final_score: {final_scores.avg:.5f}, ' + \\\n",
    "                        f'time: {(time.time() - t):.5f}', end='\\r'\n",
    "                    )\n",
    "            \n",
    "            targets = targets.to(self.device).float()\n",
    "            images = images.to(self.device).float()\n",
    "            batch_size = images.shape[0]\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                scaler.scale(loss).backward()\n",
    "\n",
    "                #adaptive_clip_grad(self.model.parameters(), clip_factor=0.01, eps=1e-3, norm_type=2.0)\n",
    "\n",
    "                final_scores.update(targets, outputs)\n",
    "                summary_loss.update(loss.detach().item(), batch_size)\n",
    "\n",
    "                scaler.step(self.optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "                if self.config.step_scheduler:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "        return summary_loss, final_scores\n",
    "    \n",
    "    def save(self, path):\n",
    "        self.model.eval()\n",
    "        torch.save({\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            'best_summary_loss': self.best_summary_loss,\n",
    "            'epoch': self.epoch,\n",
    "        }, path)\n",
    "\n",
    "    def load(self, path):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        self.best_summary_loss = checkpoint['best_summary_loss']\n",
    "        self.epoch = checkpoint['epoch'] + 1\n",
    "        \n",
    "    def log(self, message):\n",
    "        if self.config.verbose:\n",
    "            print(message)\n",
    "        with open(self.log_path, 'a+') as logger:\n",
    "            logger.write(f'{message}\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "#model def\n",
    "import timm\n",
    "\n",
    "class Alaska_NfNet(torch.nn.Module):\n",
    "    def __init__(self, model_name='nfnet_f0', out_dim=4, pretrained=False, pool=True):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained)\n",
    "        n_features = self.model.head.fc.in_features\n",
    "        self.pool = pool\n",
    "\n",
    "        if self.pool:\n",
    "            self.model.head.global_pool = nn.Identity()\n",
    "            self.model.head.fc = nn.Identity()\n",
    "            self.pooling = nn.AdaptiveAvgPool2d(1)\n",
    "            self.fc = nn.Linear(n_features, out_dim, bias=True)\n",
    "        else:\n",
    "            self.model.head.fc = nn.Linear(n_features, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.model(x)\n",
    "\n",
    "        if self.pool:\n",
    "            bs = x.size(0)\n",
    "            pooled_features = self.pooling(features).view(bs, -1)\n",
    "            features = self.fc(pooled_features)\n",
    "        return features\n",
    "\n",
    "class Alaska_Efficientnet(torch.nn.Module):\n",
    "    def __init__(self, model_name='efficientnet_b0', out_dim=4, pretrained=False, pool=True):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained)\n",
    "        n_features = self.model.classifier.in_features\n",
    "        self.pool = pool\n",
    "\n",
    "        if self.pool:\n",
    "            self.model.classifier.global_pool = nn.Identity()\n",
    "            self.model.classifier.fc = nn.Identity()\n",
    "            self.pooling = nn.AdaptiveAvgPool2d(1)\n",
    "            self.fc = nn.Linear(n_features, out_dim, bias=True)\n",
    "        else:\n",
    "            self.model.classifier = nn.Linear(n_features, out_dim, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.model(x)\n",
    "\n",
    "        if self.pool:\n",
    "            bs = x.size(0)\n",
    "            pooled_features = self.pooling(features).view(bs, -1)\n",
    "            features = self.fc(pooled_features)\n",
    "        return features\n",
    "\n",
    "#net = Alaska_NfNet(model_name='dm_nfnet_f0', pool=False, pretrained=True).cuda()\n",
    "#net = Alaska_NfNet(model_name='nfnet_f0s', pretrained=False, pool=False).cuda()\n",
    "#net = Alaska_NfNet(model_name='efficientnet_b0', pretrained=True).cuda()\n",
    "\n",
    "net = Alaska_Efficientnet(model_name=\"efficientnet_b0\", pool=False, pretrained=True).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainGlobalConfig:\n",
    "    num_workers = 16\n",
    "    batch_size = 16 \n",
    "    n_epochs = 25\n",
    "    lr = 0.001\n",
    "\n",
    "    verbose = True\n",
    "    verbose_step = 1\n",
    "\n",
    "    step_scheduler = False\n",
    "    validation_scheduler = True\n",
    "\n",
    "    SchedulerClass = torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "    scheduler_params = dict(\n",
    "        mode='min',\n",
    "        factor=0.5,\n",
    "        patience=1,\n",
    "        verbose=False, \n",
    "        threshold=0.0001,\n",
    "        threshold_mode='abs',\n",
    "        cooldown=0, \n",
    "        min_lr=1e-8,\n",
    "        eps=1e-08\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from catalyst.data.sampler import BalanceClassSampler\n",
    "\n",
    "def run_training():\n",
    "    device = torch.device('cuda:0')\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        sampler=BalanceClassSampler(labels=train_dataset.get_labels(), mode=\"downsampling\"),\n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "        pin_memory=False,\n",
    "        drop_last=True,\n",
    "        num_workers=TrainGlobalConfig.num_workers,\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        validation_dataset, \n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "        num_workers=TrainGlobalConfig.num_workers,\n",
    "        shuffle=False,\n",
    "        sampler=SequentialSampler(validation_dataset),\n",
    "        pin_memory=False,\n",
    "    )\n",
    "\n",
    "    fitter = Fitter(model=net, device=device, config=TrainGlobalConfig)\n",
    "    fitter.fit(train_loader, val_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitter prepared. Device is cuda:0\n",
      "\n",
      "2021-02-20T06:42:25.141811\n",
      "LR: 0.001\n",
      "[RESULT]: Train. Epoch: 0, summary_loss: 1.13174, final_score: 0.75188, time: 2711.75647\n",
      "[RESULT]: Val. Epoch: 0, summary_loss: 1.05157, final_score: 0.78835, time: 221.92544\n",
      "\n",
      "2021-02-20T07:31:18.998746\n",
      "LR: 0.001\n",
      "[RESULT]: Train. Epoch: 1, summary_loss: 1.04025, final_score: 0.78686, time: 2673.41059\n",
      "[RESULT]: Val. Epoch: 1, summary_loss: 1.01297, final_score: 0.79688, time: 214.36588\n",
      "\n",
      "2021-02-20T08:19:26.945541\n",
      "LR: 0.001\n",
      "[RESULT]: Train. Epoch: 2, summary_loss: 1.01792, final_score: 0.79425, time: 2610.67108\n",
      "[RESULT]: Val. Epoch: 2, summary_loss: 0.98990, final_score: 0.80749, time: 214.09589\n",
      "\n",
      "2021-02-20T09:06:31.882272\n",
      "LR: 0.001\n",
      "[RESULT]: Train. Epoch: 3, summary_loss: 1.00323, final_score: 0.79972, time: 2612.71890\n",
      "[RESULT]: Val. Epoch: 3, summary_loss: 0.99532, final_score: 0.80400, time: 213.93421\n",
      "\n",
      "2021-02-20T09:53:38.628512\n",
      "LR: 0.001\n",
      "[RESULT]: Train. Epoch: 4, summary_loss: 0.99259, final_score: 0.80302, time: 2612.08184\n",
      "[RESULT]: Val. Epoch: 4, summary_loss: 0.97102, final_score: 0.81050, time: 213.93226\n",
      "\n",
      "2021-02-20T10:40:44.817600\n",
      "LR: 0.001\n",
      "[RESULT]: Train. Epoch: 5, summary_loss: 0.98708, final_score: 0.80447, time: 2609.60769\n",
      "[RESULT]: Val. Epoch: 5, summary_loss: 0.97391, final_score: 0.80855, time: 213.76899\n",
      "\n",
      "2021-02-20T11:27:48.284708\n",
      "LR: 0.001\n",
      "[RESULT]: Train. Epoch: 6, summary_loss: 0.98191, final_score: 0.80666, time: 2611.87392\n",
      "[RESULT]: Val. Epoch: 6, summary_loss: 0.95929, final_score: 0.81530, time: 214.22160\n",
      "\n",
      "2021-02-20T12:14:54.555241\n",
      "LR: 0.001\n",
      "[RESULT]: Train. Epoch: 7, summary_loss: 0.97700, final_score: 0.80836, time: 2611.98867\n",
      "[RESULT]: Val. Epoch: 7, summary_loss: 0.96648, final_score: 0.81239, time: 214.31432\n",
      "\n",
      "2021-02-20T13:02:00.951775\n",
      "LR: 0.001\n",
      "[RESULT]: Train. Epoch: 8, summary_loss: 0.97443, final_score: 0.80954, time: 2613.91687\n",
      "[RESULT]: Val. Epoch: 8, summary_loss: 0.95757, final_score: 0.81742, time: 214.79582\n",
      "\n",
      "2021-02-20T13:49:09.841036\n",
      "LR: 0.001\n",
      "[RESULT]: Train. Epoch: 9, summary_loss: 0.97224, final_score: 0.80935, time: 2645.32548\n",
      "[RESULT]: Val. Epoch: 9, summary_loss: 0.97881, final_score: 0.80967, time: 218.45665\n",
      "\n",
      "2021-02-20T14:36:53.722300\n",
      "LR: 0.001\n",
      "Train Step 2105/15000, summary_loss: 0.97317, final_score: 0.81129, time: 352.12090\r"
     ]
    }
   ],
   "source": [
    "run_training()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "02a9428e86e648278863828c8415ba06": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5e3894c960204a5b9945ed321e529f72": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "5e5ee2bb939d4d5592a295db929297c0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "6115b6738a03430d82118c6871c2793f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_02a9428e86e648278863828c8415ba06",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_5e5ee2bb939d4d5592a295db929297c0",
       "value": " 35.1M/35.1M [00:06&lt;00:00, 5.43MB/s]"
      }
     },
     "6d75db7ce15e4a0cb47bec8af2c84925": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ca49c5d7180f4d64ba9756aeb03c7e5f",
       "max": 3.6804509E7,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_5e3894c960204a5b9945ed321e529f72",
       "value": 3.6804509E7
      }
     },
     "9806b1a81207410d85cd625c591c04e1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ca49c5d7180f4d64ba9756aeb03c7e5f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e3bce2515e7f401580708eb86cf3cacb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6d75db7ce15e4a0cb47bec8af2c84925",
        "IPY_MODEL_6115b6738a03430d82118c6871c2793f"
       ],
       "layout": "IPY_MODEL_9806b1a81207410d85cd625c591c04e1"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}